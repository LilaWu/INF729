{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hello World ! from Preprocessor\n",
      "\n",
      "\n",
      "+--------------+--------------------+--------------------+-----+--------------------+------------+--------+---------+-------------+-----------+--------------------+\n",
      "|    project_id|                name|                desc| goal|            keywords|final_status|country2|currency2|days_campaign|hours_prepa|                text|\n",
      "+--------------+--------------------+--------------------+-----+--------------------+------------+--------+---------+-------------+-----------+--------------------+\n",
      "|kkst1451568084| drawing for dollars|I like drawing pi...|   20| drawing-for-dollars|           1|      US|      USD|            9|         37|drawing for dolla...|\n",
      "|kkst1474482071|Sponsor Dereck Bl...|I  Dereck Blackbu...|  300|sponsor-dereck-bl...|           0|      US|      USD|           17|        256|sponsor dereck bl...|\n",
      "| kkst183622197|       Mr. Squiggles|So I saw darkpony...|   30|        mr-squiggles|           0|      US|      USD|           10|         13|mr. squiggles so ...|\n",
      "| kkst597742710|Help me write my ...|Do your part to h...|  500|help-me-write-my-...|           1|      US|      USD|           30|         49|help me write my ...|\n",
      "|kkst1913131122|Support casting m...|I m nearing compl...| 2000|support-casting-m...|           0|      US|      USD|           30|         44|support casting m...|\n",
      "|kkst1085176748|        daily digest|I m a fledgling v...|  700|        daily-digest|           0|      US|      USD|           32|       6894|daily digest i m ...|\n",
      "|kkst1468954715|iGoozex - Free iP...|I am an independe...|  250|igoozex-free-ipho...|           0|      US|      USD|           25|        186|igoozex - free ip...|\n",
      "| kkst194050612|Drive A Faster Ca...|Drive A Faster Ca...| 1000|drive-a-faster-ca...|           1|      US|      USD|           31|        163|drive a faster ca...|\n",
      "| kkst708883590|\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"...|Opening Friday  J...| 5000|lostles-at-tinys-...|           0|      US|      USD|           33|       1096|\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"...|\n",
      "| kkst890976740|Choose Your Own A...|This project is f...| 3500|choose-your-own-a...|           0|      US|      USD|           31|         88|choose your own a...|\n",
      "|kkst2053381363|Anatomy of a Cred...|I am an independe...|30000|anatomy-of-a-cred...|           0|      US|      USD|           37|       3779|anatomy of a cred...|\n",
      "| kkst918550886|No-bit: An artist...|I want to create ...|  300|no-bit-an-artist-...|           0|      US|      USD|           29|         96|no-bit: an artist...|\n",
      "| kkst934689279|Indie Nerd Board ...|pictured here is ...| 1500|indie-nerd-board-...|           1|      US|      USD|           31|         89|indie nerd board ...|\n",
      "| kkst191414809|Icons for your iP...|I make cool icons...|  500|awesome-icons-for...|           1|      US|      USD|           47|         79|icons for your ip...|\n",
      "| kkst569584443|HAPPY VALLEY: Dex...|I am a profession...|  500|help-me-make-my-w...|           0|      US|      USD|           40|       4353|happy valley: dex...|\n",
      "| kkst485555421|       Project Pedal|Project Pedal is ...| 1000|       project-pedal|           1|      US|      USD|           34|        131|project pedal pro...|\n",
      "|kkst1537563608|Frank Magazine Er...|We are throwing a...|  600|frank-magazine-er...|           0|      US|      USD|           16|       2716|frank magazine er...|\n",
      "|kkst1261713500|  Crossword Puzzles!|I create crosswor...| 1500|   crossword-puzzles|           1|      US|      USD|           62|        139|crossword puzzles...|\n",
      "| kkst910550425|Run, Blago Run! Show|A 3-day pop-up ar...| 3500|  run-blago-run-show|           0|      US|      USD|           25|       1476|run, blago run! s...|\n",
      "| kkst139451001|It Might Become a...|We are broke film...| 1000|it-might-become-a...|           1|      US|      USD|           37|       5736|it might become a...|\n",
      "+--------------+--------------------+--------------------+-----+--------------------+------------+--------+---------+-------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.sql.{Column, ColumnName, DataFrame, SparkSession}\n",
       "import org.apache.spark.sql.functions.udf\n",
       "import org.apache.spark.sql.functions.when\n",
       "import org.apache.spark.sql.functions.datediff\n",
       "import org.apache.spark.sql.functions.round\n",
       "import org.apache.spark.sql.functions.from_unixtime\n",
       "import org.apache.spark.sql.functions.concat_ws\n",
       "import org.apache.spark.sql.functions.lower\n",
       "conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@48a86ece\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@d5d32bf\n",
       "import spark.implicits._\n",
       "df: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 12 more fields]\n",
       "dfCasted: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 12 more fields]\n",
       "..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.{Column, ColumnName, DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.functions.udf\n",
    "import org.apache.spark.sql.functions.when\n",
    "import org.apache.spark.sql.functions.datediff\n",
    "import org.apache.spark.sql.functions.round\n",
    "import org.apache.spark.sql.functions.from_unixtime\n",
    "import org.apache.spark.sql.functions.concat_ws\n",
    "import org.apache.spark.sql.functions.lower\n",
    "\n",
    "\n",
    "\n",
    "// Des réglages optionnels du job spark. Les réglages par défaut fonctionnent très bien pour ce TP.\n",
    "// On vous donne un exemple de setting quand même\n",
    "val conf = new SparkConf().setAll(Map(\n",
    "  \"spark.scheduler.mode\" -> \"FIFO\",\n",
    "  \"spark.speculation\" -> \"false\",\n",
    "  \"spark.reducer.maxSizeInFlight\" -> \"48m\",\n",
    "  \"spark.serializer\" -> \"org.apache.spark.serializer.KryoSerializer\",\n",
    "  \"spark.kryoserializer.buffer.max\" -> \"1g\",\n",
    "  \"spark.shuffle.file.buffer\" -> \"32k\",\n",
    "  \"spark.default.parallelism\" -> \"12\",\n",
    "  \"spark.sql.shuffle.partitions\" -> \"12\"\n",
    "))\n",
    "\n",
    "// Initialisation du SparkSession qui est le point d'entrée vers Spark SQL (donne accès aux dataframes, aux RDD,\n",
    "// création de tables temporaires, etc., et donc aux mécanismes de distribution des calculs)\n",
    "val spark = SparkSession\n",
    "  .builder\n",
    "  .config(conf)\n",
    "  .appName(\"TP Spark : Preprocessor\")\n",
    "  .getOrCreate()\n",
    "import spark.implicits._ //if don't import here then compiler doesn't understand the $ before \"goal, deadline,...\"\n",
    "\n",
    "/** *****************************************************************************\n",
    "  *\n",
    "  * TP 2\n",
    "  *\n",
    "  *       - Charger un fichier csv dans un dataFrame\n",
    "  *       - Pre-processing: cleaning, filters, feature engineering => filter, select, drop, na.fill, join, udf, distinct, count, describe, collect\n",
    "  *       - Sauver le dataframe au format parquet\n",
    "  *\n",
    "  * if problems with unimported modules => sbt plugins update\n",
    "  *\n",
    "  * *******************************************************************************/\n",
    "println(\"\\n\")\n",
    "println(\"Hello World ! from Preprocessor\")\n",
    "println(\"\\n\")\n",
    "\n",
    "\n",
    "val df: DataFrame = spark\n",
    "  .read\n",
    "  .option(\"header\", true) // utilise la première ligne du (des) fichier(s) comme header\n",
    "  .option(\"inferSchema\", \"true\") // pour inférer le type de chaque colonne (Int, String, etc.)\n",
    "  .csv(\"src/main/resources/train/train_clean.csv\")\n",
    "\n",
    "val dfCasted: DataFrame = df\n",
    "  .withColumn(\"goal\", $\"goal\".cast(\"Int\"))\n",
    "  .withColumn(\"deadline\", $\"deadline\".cast(\"Int\"))\n",
    "  .withColumn(\"state_changed_at\", $\"state_changed_at\".cast(\"Int\"))\n",
    "  .withColumn(\"created_at\", $\"created_at\".cast(\"Int\"))\n",
    "  .withColumn(\"launched_at\", $\"launched_at\".cast(\"Int\"))\n",
    "  .withColumn(\"backers_count\", $\"backers_count\".cast(\"Int\"))\n",
    "  .withColumn(\"final_status\", $\"final_status\".cast(\"Int\"))\n",
    "\n",
    "\n",
    "val df2: DataFrame = dfCasted.drop(\"disable_communication\")\n",
    "val dfNoFutur: DataFrame = df2.drop(\"backers_count\", \"state_changed_at\")\n",
    "\n",
    "def cleanCountry(country: String, currency: String): String = {\n",
    "  if (country == \"False\")\n",
    "    currency\n",
    "  else\n",
    "    country\n",
    "}\n",
    "\n",
    "def cleanCurrency(currency: String): String = {\n",
    "  if (currency != null && currency.length != 3)\n",
    "    null\n",
    "  else\n",
    "    currency\n",
    "}\n",
    "\n",
    "val cleanCountryUdf = udf(cleanCountry _)\n",
    "val cleanCurrencyUdf = udf(cleanCurrency _)\n",
    "\n",
    "val dfCountry: DataFrame = dfNoFutur\n",
    "  .withColumn(\"country2\", cleanCountryUdf($\"country\", $\"currency\"))\n",
    "  .withColumn(\"currency2\", cleanCurrencyUdf($\"currency\"))\n",
    "  .drop(\"country\", \"currency\")\n",
    "\n",
    "val dfFinal: DataFrame = dfCountry\n",
    "  .withColumn(\"final_status\", when($\"final_status\".isNull || $\"final_status\" =!= 1, 0).otherwise($\"final_status\"))\n",
    "\n",
    "val dfCampagne: DataFrame = dfFinal\n",
    "  .withColumn(\"days_campaign\", datediff(from_unixtime($\"deadline\"), from_unixtime($\"created_at\")))\n",
    "  .withColumn(\"hours_prepa\", round($\"launched_at\" / 60 - $\"created_at\" / 60))\n",
    "  .withColumn(\"days_campaign\", $\"days_campaign\".cast(\"Int\"))\n",
    "  .withColumn(\"hours_prepa\", $\"hours_prepa\".cast(\"Int\"))\n",
    "  .withColumn(\"goal\", $\"goal\".cast(\"Int\"))\n",
    "\n",
    "val dfNoTime: DataFrame = dfCampagne\n",
    "  .drop(\"launched_at\", \"created_at\", \"deadline\")\n",
    "  .withColumn(\"text\", lower(concat_ws(\" \", $\"name\", $\"desc\", $\"keywords\")))\n",
    "\n",
    "val monDataFrameFinal: DataFrame = dfNoTime\n",
    "  .withColumn(\"days_campaign\", when($\"days_campaign\".isNull,-1).otherwise($\"days_campaign\"))\n",
    "  .withColumn(\"hours_prepa\", when($\"hours_prepa\" .isNull,-1).otherwise($\"hours_prepa\"))\n",
    "  .withColumn(\"goal\", when($\"goal\".isNull,-1).otherwise($\"goal\"))\n",
    "  .withColumn(\"currency2\", when($\"currency2\".isNull,\"unknown\").otherwise($\"currency2\"))\n",
    "  .withColumn(\"country2\", when($\"country2\".isNull,\"unknown\").otherwise($\"country2\"))\n",
    "\n",
    "//dfNoTime.select(\"days_campaign\").groupBy(\"days_campaign\").count().show(100) //to see if days_compaign is converted\n",
    "\n",
    "monDataFrameFinal.show()\n",
    "monDataFrameFinal.write.parquet(\"src/main/resources/preprocessed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----+\n",
      "|final_status|predictions|count|\n",
      "+------------+-----------+-----+\n",
      "|           1|        0.0|  792|\n",
      "|           0|        1.0| 3921|\n",
      "|           1|        1.0| 2559|\n",
      "|           0|        0.0| 3425|\n",
      "+------------+-----------+-----+\n",
      "\n",
      "(Le f1 score pour le test WithSimplePredictions est: ,0.569911631467866)\n",
      "(Le f1 score pour le test WithGridPredictions est:,0.5685901480529418)\n",
      "Nous pouvons apercevoir que le score du test WithGridPredictions est mieux que le score du test WithSimplePredictions. Maintenant on va voir si le score du test WithGridPredictions atteind sa maximum, on va ajouter plus de parametres: \n",
      "(Le f1 score pour le test WithSimplePredictions est: ,0.569911631467866)\n",
      "(Le f1 score pour le test WithGridPredictions est:,0.5685901480529418)\n",
      "Nous pouvons apercevoir que le score pour le test WithGridPredictions est toujours pareil. Cela vaut dire que la modele atteind tres probablement la meilleure performance sur les paramtres précédents.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.feature.RegexTokenizer\n",
       "import org.apache.spark.ml.feature.StopWordsRemover\n",
       "import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}\n",
       "import org.apache.spark.ml.feature.CountVectorizer\n",
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@7e05f99\n",
       "spark: org.apache.spa..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}\n",
    "import org.apache.spark.ml.feature.CountVectorizer\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.{Pipeline}\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "\n",
    "\n",
    "val conf = new SparkConf().setAll(Map(\n",
    "  \"spark.scheduler.mode\" -> \"FIFO\",\n",
    "  \"spark.speculation\" -> \"false\",\n",
    "  \"spark.reducer.maxSizeInFlight\" -> \"48m\",\n",
    "  \"spark.serializer\" -> \"org.apache.spark.serializer.KryoSerializer\",\n",
    "  \"spark.kryoserializer.buffer.max\" -> \"1g\",\n",
    "  \"spark.shuffle.file.buffer\" -> \"32k\",\n",
    "  \"spark.default.parallelism\" -> \"12\",\n",
    "  \"spark.sql.shuffle.partitions\" -> \"12\",\n",
    "  \"spark.driver.maxResultSize\" -> \"2g\"\n",
    "))\n",
    "\n",
    "val spark = SparkSession\n",
    "  .builder\n",
    "  .config(conf)\n",
    "  .appName(\"TP Spark : Trainer\")\n",
    "  .getOrCreate()\n",
    "\n",
    "/*******************************************************************************\n",
    "  *\n",
    "  *       TP 3\n",
    "  *\n",
    "  *       - lire le fichier sauvegarder précédemment\n",
    "  *       - construire les Stages du pipeline, puis les assembler\n",
    "  *       - trouver les meilleurs hyperparamètres pour l'entraînement du pipeline avec une grid-search\n",
    "  *       - Sauvegarder le pipeline entraîné\n",
    "  *\n",
    "  *       if problems with unimported modules => sbt plugins update\n",
    "  *\n",
    "  ********************************************************************************/\n",
    "\n",
    "\n",
    "val df = spark.read.load(\"src/main/resources/preprocessed\")\n",
    "//df.show(100)\n",
    "\n",
    "//Stage1: récupérer les mots des textes\n",
    "val tokenizer = new RegexTokenizer()\n",
    "  .setPattern(\"\\\\W+\")\n",
    "  .setGaps(true)\n",
    "  .setInputCol(\"text\") //input is a ligne of words\n",
    "  .setOutputCol(\"tokens\") //output is a list of seperated words\n",
    "\n",
    "//val regexTokenized = tokenizer.transform(df)\n",
    "\n",
    "//Stage 2 : retirer les stop words\n",
    "val remover = new StopWordsRemover()\n",
    "  .setInputCol(\"tokens\")\n",
    "  .setOutputCol(\"removed\") //output is a list of seperated words without propositions etc...\n",
    "\n",
    "//val stopremoved = remover.transform(regexTokenized)\n",
    "\n",
    "//Stage 3 : computer la partie TF\n",
    "val vectorizer = new CountVectorizer()\n",
    "  .setInputCol(\"removed\")\n",
    "  .setOutputCol(\"vector\") //output is a array of a int and two vectors of index and frequency of words\n",
    "  .setVocabSize(100)\n",
    "  .setMinDF(2)\n",
    "\n",
    "//val vectorized = vectorizer.fit(stopremoved).transform(stopremoved)\n",
    "\n",
    "//Stage 4 : computer la partie IDF\n",
    "val idf = new IDF()\n",
    "  .setInputCol(\"vector\")\n",
    "  .setOutputCol(\"tfidf\") //output is the tfidf of vector\n",
    "\n",
    "//val idfModel = idf.fit(vectorized).transform(vectorized)\n",
    "\n",
    "//Stage 5 : convertir country2 en quantités numériques\n",
    "val indexercountry = new StringIndexer()\n",
    "  .setInputCol(\"country2\")\n",
    "  .setOutputCol(\"country_indexed\") //output gives country a index\n",
    "  .setHandleInvalid(\"keep\")\n",
    "\n",
    "//val countryindexed = indexercountry.fit(idfed).transform(idfed)\n",
    "\n",
    "//Stage 6 : convertir currency2 en quantités numériques\n",
    "val indexercurrency = new StringIndexer()\n",
    "  .setInputCol(\"currency2\")\n",
    "  .setOutputCol(\"currency_indexed\") //output gives currency a index\n",
    "  .setHandleInvalid(\"keep\")\n",
    "\n",
    "//val currencyindexed = indexercurrency.fit(countryindexed).transform(countryindexed)\n",
    "\n",
    "//Stages 7 et 8: One-Hot encoder ces deux catégories\n",
    "val encoder = new OneHotEncoderEstimator()\n",
    "  .setInputCols(Array(\"country_indexed\", \"currency_indexed\"))\n",
    "  .setOutputCols(Array(\"country_onehot\", \"currency_onehot\")) //output convert into onehout vectors\n",
    "\n",
    "//val encoded = encoder.fit(currencyindexed).transform(currencyindexed)\n",
    "\n",
    "//Stage 9 : assembler tous les features en un unique vecteur\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"tfidf\", \"days_campaign\", \"hours_prepa\", \"goal\", \"country_onehot\", \"currency_onehot\"))\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "//Stage 10 : créer/instancier le modèle de classification\n",
    "val lr = new LogisticRegression()\n",
    "  .setElasticNetParam(0.0)\n",
    "  .setFitIntercept(true)\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setLabelCol(\"final_status\")\n",
    "  .setStandardization(true)\n",
    "  .setPredictionCol(\"predictions\")\n",
    "  .setRawPredictionCol(\"raw_predictions\")\n",
    "  .setThresholds(Array(0.7, 0.3))\n",
    "  .setTol(1.0e-6)\n",
    "  .setMaxIter(20)\n",
    "\n",
    "//Stage 11: création du Pipeline\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(tokenizer,remover,vectorizer,idf,indexercountry,indexercurrency,encoder, assembler,lr))\n",
    "\n",
    "//Stage 12: split des données en training et test sets\n",
    "val Array(train, test) = df.randomSplit(Array(0.9, 0.1), seed = 12345)\n",
    "\n",
    "//Stage 13: entraînement du modèle\n",
    "val model = pipeline.fit(train)\n",
    "\n",
    "//Stage 14: Test du modèle\n",
    "val dfWithSimplePredictions = model.transform(test)\n",
    "\n",
    "dfWithSimplePredictions.groupBy(\"final_status\", \"predictions\").count.show()\n",
    "\n",
    "val evalution = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"final_status\")\n",
    "  .setMetricName(\"f1\")\n",
    "  .setPredictionCol(\"predictions\")\n",
    "\n",
    "//Stage 15: réglage des hyper-paramètres (a.k.a. tuning) du modèle - Grid search\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(lr.regParam, Array(10e-8, 10e-6, 10e-4, 10e-2))\n",
    "  .addGrid(vectorizer.minDF,Array(55.0,75.0,95.0))\n",
    "  .build()\n",
    "\n",
    "val trainValidationSplit = new TrainValidationSplit()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(evalution)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setTrainRatio(0.7)\n",
    "  .setParallelism(2)\n",
    "\n",
    "val model_grid = trainValidationSplit.fit(train)\n",
    "\n",
    "//Stage 16: réglage des hyper-paramètres (a.k.a. tuning) du modèle - Test du modèle\n",
    "val dfWithGridPredictions = model_grid.transform(test)\n",
    "\n",
    "println(\"Le f1 score pour le test WithSimplePredictions est: \", evalution.evaluate(dfWithSimplePredictions))\n",
    "println(\"Le f1 score pour le test WithGridPredictions est:\", evalution.evaluate(dfWithGridPredictions))\n",
    "println(\"Nous pouvons apercevoir que le score du test WithGridPredictions est mieux que le score du test WithSimplePredictions. Maintenant on va voir si le score du test WithGridPredictions atteind sa maximum, on va ajouter plus de parametres: \")\n",
    "\n",
    "val paramGrid2 = new ParamGridBuilder()\n",
    "  .addGrid(lr.regParam, Array(10e-8, 10e-7, 10e-6, 10e-5, 10e-4, 10e-3, 10e-2))\n",
    "  .addGrid(vectorizer.minDF,Array(55.0,60.0,70.0,75.0,80.0,85.0,90.0,95.0))\n",
    "  .build()\n",
    "\n",
    "val trainValidationSplit2 = new TrainValidationSplit()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(evalution)\n",
    "  .setEstimatorParamMaps(paramGrid2)\n",
    "  .setTrainRatio(0.7)\n",
    "  .setParallelism(2)\n",
    "\n",
    "val model_grid2 = trainValidationSplit2.fit(train)\n",
    "\n",
    "val dfWithGridPredictions2 = model_grid2.transform(test)\n",
    "\n",
    "println(\"Le f1 score pour le test WithSimplePredictions est: \", evalution.evaluate(dfWithSimplePredictions))\n",
    "println(\"Le f1 score pour le test WithGridPredictions est:\", evalution.evaluate(dfWithGridPredictions))\n",
    "println(\"Nous pouvons apercevoir que le score pour le test WithGridPredictions est toujours pareil. Cela vaut dire que la modele atteind tres probablement la meilleure performance sur les paramtres précédents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
